<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Diana Huang - Machine Learning</title>
        <!-- Favicon-->
        <link rel="icon" type="image/x-icon" href="../assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Merriweather+Sans:400,700" rel="stylesheet" />
        <link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="../../css/styles.css" rel="stylesheet" />
        <!-- Custom font -->
        <link href="../../css/font.css" rel="stylesheet" />

        <!-- MathJax -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-light fixed-top py-3" id="mainNav">
            <div class="container">
                <a class="navbar-brand js-scroll-trigger" href="#page-top">DH</a><button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
                <div class="collapse navbar-collapse" id="navbarResponsive">
                    <ul class="navbar-nav ml-auto my-2 my-lg-0">
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="../index.html">Blog</a></li>
                        <li class="nav-item"><a class="nav-link js-scroll-trigger" href="../../index.html">Portfolio</a></li>
                    </ul>
                </div>
            </div>
        </nav>
        <!-- Welcome -->
        <section class="page-section" id="article">
            <div class="container">
                <h2 class="text-left mt-0">Linear Regression</h2>
                <br>
                <p class="mb-0">
                    Recall supervised learning from <a href="ml1.html">Lecture 1</a>. The goal is to predict the label \(\hat{y} \in \mathbb{R}\) from the d-dimensional features \(x \in \mathbb{R}^d\).
                    
                    <br><br>
                    The linear regression model is a supervised learning model and is defined as follows:
                    $$\hat{y} = w_1^Tx + w_2$$
                    $$x = \begin{bmatrix}x_1\\.\\.\\x_d\end{bmatrix} \quad  w_1 = \begin{bmatrix}w_1\\.\\.\\w_d\end{bmatrix}$$
                    $$\hat{y} = (\sum_{j=1}^d w_{1, j}x_j)+w_2$$

                    \(w_1\) is the weight assigned to the inputs to determine how heavily each input affects the prediction. \(w_2\) is the bias term to offset predictions on the y-axis. The learning objective is to choose \(w_1\) and \(w_2\) based on the dataset.
                    So how do we make that choice? We use a loss function to measure how well a model is doing. This function measures the error in our predictions.

                    <br><br>
                    For linear regression, we use the squared loss function:
                    $$\frac{1}{N}\sum_{i=1}^N\frac{1}{2}(\hat{y}^i-y^i)^2$$
                    $$=\frac{1}{N}\sum_{i=1}^N\frac{1}{2}(w_1^Tx + w_2-y^i)^2$$
                    At this point in the lecture, I was wondering why we had the \(\frac{1}{2}\) term in the summation since without it, it's the average of squared error which makes sense as a loss function.
                    Later in the lecture, we do some derivations on this function where a 2 term pops out. The \(\frac{1}{2}\) is there to benefit our future selves. It also does not affect the loss function since it's a scalar term.

                    <br><br>
                    We want to pick \(w_1\) and \(w_2\) such that the squared loss function is minimized:
                    $$\underset{w_1 \in \mathbb{R}^d, w_2 \in \mathbb{R}}{\operatorname{argmin}}\frac{1}{N}\sum_{i=1}^N\frac{1}{2}(w_1^Tx + w_2-y^i)^2$$
                    If we convert the squared loss function from a scalar error problem to a matrix problem, the math is cleaner:
                    $$Y:=\begin{bmatrix}y^1\\.\\.\\y^N\end{bmatrix}\in \mathbb{R}^N \qquad W := \begin{bmatrix}w_{1, 1}\\.\\.\\w_{1,d}\\w_2\end{bmatrix}\in \mathbb{R}^{d+1} \qquad X:= \begin{bmatrix}\cdots & x_1 & \cdots & 1\\ & \vdots & & \vdots\\ \cdots & x_N & \cdots & 1\end{bmatrix}\in \mathbb{R}^{N\times(d+1)}$$
                    $$\underset{W \in \mathbb{R}^{d+1}}{\operatorname{argmin}}\frac{1}{N}||XW-Y||^2_2$$
                    By tacking on the column full of 1's in \(X\), we are able to account for the bias term \(w_2\) in a more compact way. In order to find the value of \(W\) that minimizes the new matrix error, we can use the gradient of W and set it to 0:

                    $$\frac{\partial \frac{1}{2}||XW-Y||^2_2}{\partial W} = 0$$
                    $$\frac{\partial}{\partial W} \frac{1}{2} [W^TX^TXW - 2Y^TXW + Y^TY] = 0$$
                    $$\frac{1}{2}[2X^TXW - 2Y^TX] = 0$$
                    $$X^TXW - Y^TX = 0$$
                    $$X^TXW = X^TY$$
                    $$\hat{W} = (X^TX)^{-1}X^TY$$
                    So \(\hat{W}\) is the ordinary least squares (OLS) estimator. So \(X^TX\hat{W} = X^TY\) is the derivative condition that \(\hat{W}\) satisfies when it exists.

                    <br><br>
                    In this lecture we assume that \(\underset{W \in \mathbb{R}^{d+1}}{\operatorname{argmin}}\frac{1}{N}||XW-Y||^2_2\) is convex as we will go into greater detail in the optimization lectures. Because it is convex, we know there is only one minimizer.
                    One weak point of this estimator is that \((X^TX)^{-1}\) must be solvable, so \(X^TX\) must be invertible. However, \(X^TX\) is sometimes badly conditioned and we must use inverse approximations like pseudo-inverse to evaluate \(X^TX\).
                    We could also use ridge regression and add a regularization term: 
                    $$\underset{W \in \mathbb{R}^{d+1}}{\operatorname{argmin}}\frac{1}{N}||XW-Y||^2_2 + \frac{\lambda}{2}||W||^2$$
                    $$\hat{W} := (X^TX + \lambda I)^{-1}X^TY$$
                </p>
            </div>
        </section>

        <!-- Footer-->
        <footer class="bg-dark py-5">
            <div class="container"><div class="small text-center text-muted">Copyright Â© 2020 - Start Bootstrap</div></div>
        </footer>
        <!-- Bootstrap core JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js"></script>
        <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.bundle.min.js"></script>
        <!-- Third party plugin JS-->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.4.1/jquery.easing.min.js"></script>
        <!-- Core theme JS-->
        <script src="../../js/scripts.js"></script>
    </body>
</html>
